{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3104a7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell a topic: Animal\n",
      "Hi, what do you want to know?\n",
      "Humans\n",
      "There is a tension between the role of animals as companions to humans, and their existence as individuals with rights of their own.\n",
      "Hi, what do you want to know?\n",
      "What about ancestors?\n",
      "6,331 groups of genes common to all living animals have been identified; these may have arisen from a single common ancestor that lived 650 million years ago.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# warnings.filterwarnings(\"ignore\", category=SomeWarningCategory)\n",
    "import nltk\n",
    "import wikipedia\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import ssl\n",
    "\n",
    "# Bypass SSL certificate verification\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# Download specific NLTK data file\n",
    "nltk.data.find('corpora/wordnet.zip')\n",
    "\n",
    "# Continue with your code that requires NLTK data\n",
    "# ...\n",
    "\n",
    "\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('punkt')\n",
    "\n",
    "\n",
    "topic= input(\"Tell a topic: \")\n",
    "text = wikipedia.page(topic).content\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemma_me(sent):\n",
    "    sentence_tokens = nltk.word_tokenize(sent.lower())\n",
    "    pos_tags = nltk.pos_tag(sentence_tokens)\n",
    "\n",
    "    sentence_lemmas = []\n",
    "    for token, pos_tag in zip(sentence_tokens, pos_tags):\n",
    "        if pos_tag[1][0].lower() in ['n', 'v', 'a', 'r']:\n",
    "            lemma = lemmatizer.lemmatize(token, pos_tag[1][0].lower())\n",
    "            sentence_lemmas.append(lemma)\n",
    "\n",
    "    return sentence_lemmas\n",
    "\n",
    "def process(text, question):\n",
    "    sentence_tokens = nltk.sent_tokenize(text)\n",
    "    sentence_tokens.append(question)\n",
    "\n",
    "    tv = TfidfVectorizer(tokenizer=lemma_me)\n",
    "    tf = tv.fit_transform(sentence_tokens)\n",
    "    values = cosine_similarity(tf[-1], tf)\n",
    "    index = values.argsort()[0][-2]\n",
    "    values_flat = values.flatten()\n",
    "    values_flat.sort()\n",
    "    coeff = values_flat[-2]\n",
    "    if coeff > 0.3:\n",
    "        return sentence_tokens[index]\n",
    "\n",
    "\n",
    "while True:\n",
    "    \n",
    "    question = input(\"Hi, what do you want to know?\\n\")\n",
    "    output = process(text, question)\n",
    "    if output:\n",
    "        print(output)\n",
    "    elif question=='quit':\n",
    "        break\n",
    "    else:\n",
    "        print(\"I don't know.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4bce0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1adea1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
